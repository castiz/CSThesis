\documentclass[midd]{thesis}

\usepackage{graphicx}
\usepackage{times}
\usepackage{longtable}
\usepackage{ctable}



\bibliographystyle{plain}

\title {Combating Fraud By Leveraging Machine Learning and Human Behavior}

\author {Casey Astiz}
\adviser {Professor Michael Linderman}

\begin{document}

\maketitle
\pagenumbering{roman}

\begin{abstract}
I am writing about fraud and how to detect it! Wooh!
\end{abstract}

\begin{acknowledgements}
I would like to thank Professor Linderman and Professor Scharstein first and foremost for their help in this Computer Science Thesis. Without them, this thesis would cease to exist. I would also like to thank the Computer Science department at Middlebury College for giving me the opportunity to learn and develop as a computer scientist. Last, I would like to thank all my unnamed interviewees who were willing to discuss their expertise about fraud detection with me. I truly appreciate all who helped create this thesis, and add to my knowledge!

\end{acknowledgements}

\contentspage
\tablelistpage   % comment this out if you don't have any tables
\figurelistpage

\normalspacing \setcounter{page}{1} \pagenumbering{arabic}

\chapter{Introduction}
\label{sec:intro}
%This thesis has many chapters.  For more on Alice see
%Chapter~\ref{sec:alice}, and in particular Section~\ref{sec:reproach}.

In today's world, there are more and more ways to pay for goods and services. Cash is a way of the past; mobile, peer to peer, and online payments are the way of the future. While these convenient payment methods are very exciting, the additional methods of payment lead to more gaps in security, in particular fraud. Fraudsters have taken advantage of every system in some form or another, and it is the financial institution's job to block or detect these fraudsters or else deal with the payments. For reference, credit card fraud through online payments in Australia hit \$476 million in 2017, rising \$58 million from the year before \cite{Wang2018}. This level of fraud is happening all over the world, and it is necessary to find more efficient and effective methods to catch fraudsters to keep everyday financial transactions safe. In addition, when banks lose money to fraudsters, card holders partially or entirely pay for this loss through higher interest rates, fees, or reduced benefits. Without automatic fraud detection, a significant amount of overhead is created because of the need to investigate these transactions \cite{Chan}. It is important to limit the amount of fraud because consumers pay for this fraud. 

The main purpose of this thesis is to investigate current methods of fraud detection and implement one or more of those methods in a financial context. I will discuss the most prevalent methods for fraud detection are currently, as well as past fraud detection methods. Fraud detection is a difficult field of study because it falls in the subset of anomaly detection and has very skewed classes as more transactions are not fraud than are fraud, and the classification of fraud is always changing. In order to build an effective fraud detector, it must be adaptive to new patterns and categories of fraud that are developing as fraudsters find new methods to deceive the system.

The main focus of this thesis will be a review of different methods used in the industry in the past and today for fraud detection, as well as my own implementation using some dataset of transactions to be determined. This background section will include information from published research papers as well as information I learned from interviewing people at different companies in this field to provide some context for the literature review. Comparing business interactions with fraud to published research in the field will allow me to somewhat analyze the performance of different machine learning methods in real life versus a research context.  

Chapter~\ref{sec:background} discusses past and current research into the field of fraud detection. Chapter~\ref{sec:context} gives context for the problem at hand, synthesizing the information I learned from interviewing professionals in the fraud world. In Chapter~\ref{sec:impl}, I describe the methodology and experiments I ran.



\pagebreak

\chapter{Modern Day Fraud}
\label{sec:background}

What exactly is fraud? Fraud is defined as ``wrongful deception with the intent to gain personally or financially," or intentionally deceiving another person to obtain something they have \cite{legaldict}. Fraud can either be a civil or criminal offense, depending on the state. There are a multitude of types of frauds, from credit card fraud and website misdirection to pyramid schemes and insurance fraud. There has been much research into the best and most efficient ways to detect different kinds of fraud. The goal of this research is to detect the most amount of fraud with the least error. This may seem obvious, but is particularly important because this research can be applied to real scenarios. It is important for business to balance finding all fraud and limiting their customers. A company may choose to accept a small level of fraudulent transactions in order to include as many real transactions as possible. For example, it is less expensive for a company to let through a new customer that is potentially fraudulent  occasionally, and maximize the number of new customers that are not fraudsters. The opportunity cost of rejecting every potential fraudster is too high for most companies to risk. For that reason, fraud detection out in the wild is never going to be at 100 percent because the false positives are too big a cost.

However, in a research context, there are no such limits. The previous fraud research has covered many different types of fraud, from broad to narrow scenarios. Researchers are able to manipulate datasets to have ideal fraud to valid transaction ratios, and find the place their algorithm performs the best. Since researchers do not need to make a split second decision for whether or not to deny a transaction, they have time to find the best algorithm for the specific problem they are working on. By examining past and current research in this field, I have created a synopsis of information about fraud detection, and will implement a similar approach to one of the papers discussed in Chapter~\ref{sec:impl}. 


\section{Credit Card Fraud Research}

An early example of credit card fraud detection comes from a study by Chan and Stolfo from 1998 \cite{Chan}. This paper, titled ``Toward Scalable Learning with Non-Uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection" seeks to find a solution to classifying skewed classes. Here, the number of fraudulent transactions is relatively small compared to the legitimate ones, and the amount of financial loss for each fraudulent transaction depends on the amount of the transaction and other factors. However, millions of transactions occur every day, and only a small portion of these transactions are fraudulent. Skewed classes are also the norm for cellular phone fraud detection as well as natural language processing problems.

In order to solve the problem, Chan and Stolfo implement a cost-sensitive approach instead of a classic error rate. They use a multi-classifier meta-learning approach, where they create data subsets with appropriate class distributions and apply learning algorithms to the subsets. Then, they integrate and optimize the cost performance of the classifiers by learning from their classification behavior. The benefit of this approach is that it handles non-uniform cost per error and is cost sensitive during the learning process. Chan and Stolfo use a dataset from Chase Manhattan Bank containing half a million transactions, from which 20 \% are fraudulent, and implemented a cost model based on information from a bank representative. 

Given this data and information, the authors experimented with the effects of training class distributions on the credit card cost model. They used the first 10 months as training data and the 12 month for testing, and randomly sampled transactions to get varied fraud amounts. The multi-classifier meta learning approach works by randomly dividing the dataset into 4 subsets, and then learns on the subsets using one of four algorithms: CART, C4.5, RIPPER, or BAYES. The meta learning then learns from that class combiner behavior and uses that as a class-combiner strategy. The authors found that the training class distribution can affect the performance of the learned classifier, which is an issue given that natural distributions can be different than desired training distribution. However, they found their method to be efficient and scalable for large datasets. Since thieves learn and fraud patterns evolve over time, and adaptive classifier selection method is essential. This modular multi-classifier approach facilitates adaptive over time and removal of out of date learned behavior.

A 1997 paper by Stolfo et al uses Meta-Learning for credit card fraud detection \cite{Stolfo1997}. Here, the authors argue that a false positive rates and true positive rates are much better metrics than overall accuracy when evaluating these learned fraud classifiers. Their proposed meta-learning system allows financial institutions to share their models of fraudulent transactions by exchanging classifier agents in a secured agent infrastructure. This is a useful tool for financial institutions to be able to work together to solve similar fraud problems. 

For this study, the authors used a database of 500,000 transaction records from the Financial Services Technology Consortium, each containing 30 fields. In their experiment, the authors tested various machine learning and meta-learning models with different proportions of fraud to non-fraud training data. Based on their experiments, they find that a 50\%/50\% distribution of fraud to non-fraud training data generates classifiers with the highest true positive rate and the lowest false positive rate. The best method the authors discovered is using meta-learning with BAYES as a ``meta-learner to combine base classifiers" \cite{Stolfo1997}. The authors' results are in line with previous research in this field. 

In a paper published this year, Zanin et al take a new approach to credit card fraud detection. Here, the authors use parenclitic network analysis, which is a hybrid data mining and complex network classification algorithm \cite{Zanin2018}. The authors find that fraud detection is a similar problem to designing a recommendation machine or diagnostic medical tools, which suggests a complex network approach may be beneficial. Here, the definition of credit card fraud is included in a wider notion of financial frauds. Like the other papers discussed, the authors emphasize the rising costs of credit card fraud, rising to billions of dollars in yearly losses, and comprising about 1.4\% of online payments. However, credit card fraud is not only important for the bottom line, as ``credit card frauds have important social consequences and ramifications, as they support organized crime, terrorism funding, and international narcotics trafficking" \cite{Zanin2018}. 

Zanin et al combine data mining techniques to detect hidden patterns in data with a networking approach, which adds the ability to synthesize metrics to describe a ``global structure created by the interactions between the different features" \cite{Zanin2018}. This study uses parenclitic networks, and evaluates them on a dataset of real transactions in comparison to the results of a standard ANN approach. In general, a parenclitic network is a network reconstruction technique that allows highlighting of the differences between one instance and a set of standard instances. These parenclitic networks are summaries of the groups of features whose correlation differs from a normal or licit transaction. Therefore, the structure of a parenclitic network stores information about abnormal correlated features in a credit card transaction. The authors take the computed network and transform it into a set of features compatible with a data mining algorithm. There are two main families of fraud detection algorithms: supervised and unsupervised learning. While both have their advantages and disadvantages, supervised learning is better at detecting illegal transactions whereas unsupervised learning is better at problems like self-organizing maps.

To test their parenclitic network approach, the authors used a dataset of all credit and debit card transactions from clients of a Spanish Bank BBVA from January 2011 to December 2012. This dataset includes standard fields about transactions such as amount, origin and destination, which has been anonymized. The authors have also synthesized information like average transaction size from a user from the given information. They use multi-layer perceptrons, a type of ANN, as the model for classifying their transactions. These multi-layer perceptrons are represented by a set of connected nodes in which each connection has a weight associated with it that can be adjusted. Zanin et al find that the parenclitic networks themselves are not enough to reach a low classification error. However, the addition of parenclitic features to the raw data set enforces the results, decreasing the error rate from 19.2\% to 12.23\% \cite{Zanin2018}. Like others have mentioned before, false positives are extremely expensive.

In a 2017 paper, Wedge et al have attempted to solve the ``false positive" problem in fraud prediction at an industrial scale. The goal of this paper is to use a feature engineering approach to dramatically reduce false positives \cite{Wedge}. It is estimated that 1 in 5 declared fraud transactions are actually fraud, and analysts have found that these false positives are costing more than the fraud itself. However, improving false positive rates with human involvement is very costly, prompting Wedge et al to study the automatic method by the name of deep feature synthesis. With so many online payments, there has been a massive increase in available transaction data, meaning that companies and banks are better equipped to handle fraud detection. The dataset the authors use contains roughly 9.5 million transactions with approximately 112,000 fraudulent transactions. The data is incredibly rich and granular, and requires ``a 59-page dictionary to describe each transaction attribute and all of its possible values" \cite{Wedge}. 

The authors create their features using only transaction information and aggregated historical information. These transactions were classified using scikit-learn's random tree classifier with 100 trees. The random forest design are helpful for this project because it allows the relative feature importances to be calculated. In addition to this random forest, the authors use a deep feature synthesis algorithm to automatically derive behavioral features based on historical data of the card. Through these methods, they were able to reduce false positives by 54\% \cite{Wedge}. Interestingly, they found that their solution can maintain similar benefits when the historical features of a card are computed once every 7 days. This eliminates the need for streaming computing because the features can be computed after a period of time instead of using a constant stream of data. While the authors' system reduced the false positive rate, it did not do significantly differently than BBVA's current system in determining false transaction, meaning that BBVA's system does well in detecting high valued fraud. However, Wedge et al's system still reduced total cost to this bank by 190,000 euros. As a conclusion, this system is helpful in reducing false positives in a way that does not increase latency to a noticeable point for users, while saving banks and firms significant amounts of money.


\subsection{Financial Fraud Research}

Though credit card fraud is under the umbrella of financial fraud, there is a large field of study devoted to financial fraud as a whole. This type of fraud can be anything related to financial transactions, and anything that a bank may be responsible for. In a 2011 paper, Johan Perols analyzed the best methods analyzes statistical and machine learning algorithms for financial statement fraud detection \cite{Perols2011}. In the United States, the cost of financial statement fraud is estimated at around \$572 billion per year. With any type of financial fraud, there is a certain level of financial burden for the company or bank that has to pay out from fraudulent actions. Other than the pure fraud costs, having high levels of fraud leads to uncertainty for both the customers and bank trustees. This uncertainty then creates higher transaction costs for all and less efficient markets. 

As is the case with credit card fraud, there is a high imbalance between fraudulent financial statements and non-fraudulent statements. With financial statements, it is much more costly to a bank or a firm to classify a fraudulent firm as a legitimate firm than it is to classify a legitimate firm as a fraud firm. This is because fraudsters will continue to cost a company or bank money until they are caught. In addition, fraudsters are actively trying to cover up their fraud, making fraud detection increasingly more difficult. For these reasons, Perols is looking for the best classification method with the most utility. The results of this paper are of particular use to institutions like the Securities and Exchange Commission as well as other auditors. In this study, Perols uses open source classification algorithms from a data mining tool called Weka \cite{Perols2011}. From Weka, six algorithms were chosen for analysis: artificial neural networks (ANN), support vector machines (SVM), C4.5, logistic regression, stacking, and bagging. The experiment was conducted using a dataset consisting of fraud investigations reported in the Accounting and Auditing Enforcement Releases from 1998 through 2005. Similar to other studies referenced in this thesis, the author enforces the necessity to not rely upon accuracy to measure the success of an algorithm for a skewed class problem. 

To test the different models, the author uses a ten-fold cross validation instead of using the same dataset as both the training set and the test or evaluation set. The results of this study are somewhat surprising and do not follow other research. Here the author finds that SVM outperforms the other classification algorithms, followed by logistic regression then bagging. This is a surprising result because previous research has found neural networks to have better results at classifying fraud. In addition, ``out of 42 predictors examined, only six are consistently selected and used by different classification algorithms: auditor turnover, total discretionary accruals, Big 4 auditor, accounts receivable, meeting or beating analyst forecasts, and unexpected employee productivity" \cite{Perols2011}. It is interesting that the different training algorithms were picking out only a small portion of the same metrics to judge these transactions on. As an extension of Perols' research, future research could examine other classification algorithms, and leverage data mining research focusing on the class imbalance problem. 


\section{Other Fraud Research}

While this thesis mainly focuses on fraud research in a credit card and online payments format, there has also been much research into fraud in other forms. However, the broader goals of this research align with the goals of credit card fraud detection, and the results are widely applicable. An example of this is a paper by Fawcett and Provost from 1996. This paper, ``Combining Data Mining and Machine Learning for Effective User Profiling," describes automatic methods for fraud detection based on profiling customer behavior \cite{Fawcett1996}. Instead of focusing on credit card data, this paper uses cellphone cloning data, which is a particularly expensive type of fraud. This is when a customer's Mobile Identification Number and Electronic Serial Number are cloned and programmed into a cellphone that does not belong to that customer. 

In this paper, the authors present a framework for automatically generating fraud detectors. Here, the model uses a lot of calls to determine fraudulent patterns then use these broader patterns and apply them to individual accounts. Some of the standard methods used to detect fraud in these calls are to search for collisions or overlapping calls between the original user and the fraudster, or to search for calls in terminal proximity that could not have possibly been placed by the same user. The profilers the paper describe capture the typical behavior of an account and calculate how far an account is from typical behavior. Given this information, the authors use rule learning program, which searches for rules with certainty factors above a user-defined threshold. Here, each account has its own set of rules, and each call has 313 attributes that allow for partitions of the calls. Through their data mining process, 3630 rules were created for detecting fraud, which were then narrowed down to 99 rules. The conclusions of this paper were that the authors had to sacrifice accuracy in order to reduce the total cost of the system, but that their method was overall effective \cite{Fawcett1996}. Like other papers, the authors highlight the need to build adaptive systems to account for ever changing fraud.

Crowdsourcing is another way fraud is introduced into daily life, introducing new types of malpractices into Internet advertising. In a paper by Tian et al, authors attempt to detect crowd fraud in internet advertising  \cite{Tian}. In this paper, authors are focusing on detection when malicious crowdsourcing platforms attack other advertisers. These types of attacks are much harder to detect than automated attacks because they are human generated crowd frauds, and ever changing. Here, fraudsters manipulate pay per click method of payments to make more money. Crowd fraud is defined by a few characteristics: moderateness, synchronicity, and dispersivity. Crowd fraud often arises from a vast number of attacking sources, but each source has a low fraudulent traffic. These attacks are meant to hurt advertisers by raising their advertising expenses. There are similar paradigms in credit card fraud, where fraudsters use multiple fake identities and credit cards to make a lot of low cost transactions. As of the time of this paper in 2015, current methods for crowd fraud detection rely on previously known knowledge and rules based on suspicious queries. However, this is very labor intensive. 

To detect crowd fraud automatically, Tian et al use an enhanced graph model based on anomaly detection methods for detection coalitions. There are a few main stages to this system. First, the authors construct a surfer-advertiser bipartie graph, where each edge represents a click log. Then, the authors look for clusters to find surfer coalitions that exhibit synchronicity, and then filter out the large coalitions. Once these large coalitions are determined, they can be removed from the domain of the centralized advertisers. Before constructing these graphs, the authors also pre-filter to remove large amounts of non-fraudulent data; in this case they remove more than 70\% of the click logs. The authors test their nonparametric clustering algorithm on real world data and find that the system does indeed converge and scale at a linear rate, with a 98.7\% accuracy in finding malicious coalitions \cite{Tian}. Through converting this coalition detection into a clustering problem, the authors are able to get very high results in the crowd fraud detection field. 

Another type of fraud in everyday places is ranking fraud for mobile apps. This ranking fraud is committed in order to move apps up the ranking lists. Here, mobile app developers are using shady means to artificially raise their app's ranking by inflating their apps' sales or posting phony app ratings. Zhu et al investigate ways of accurately locating the ranking fraud by mining the active periods, mainly focusing on detecting local anomalies versus global anomalies of app rankings \cite{Zhu2015}. With over 1.6 million apps in the Apple App Store and Google Play, app leader boards are an important marketing tool for developers. However, developers can manipulate their ratings by implementing ``bot farms" or "human water armies" which increase app statistics like downloads and ratings in a very short time. App ranking fraud detection is particularly difficult because the fraud can occur at any point of the app's life cycle, which is why the authors focus on local fraud detection instead of the global anomaly of mobile apps. Another challenge for this problem is that there are a vast number of apps and no easy way to determine the ones that have committed fraud, enforcing the need for automatic fraud detection methods. Lastly, the authors must find implicit fraud patterns as evidence for ranking fraud because of the dynamic nature of mobile app rankings.

In order to detect leader board fraud, the authors actually look for leading sessions of an app, and find that fraudulent apps' leading session ranking patterns have different characteristics of normal apps. Zhu et al use statistical hypotheses tests to find evidences for ranking fraud, then use an unsupervised evidence-aggregation method to combine the three types of evidences. The three types of evidences the authors focus on are ranking based, rating based, and review based, with each of these evidences having several factors built into them. By testing their model in different permutations of evidences, the authors find that their combination of all three evidences outperforms single evidence models and other baseline models. This can be because the app ranking fraud does not necessarily cause app rankings to increase, but may lead to higher downloads or reviews. Therefore, it is more important to look at all the factors rather than the individual evidences \cite{Zhu2015}. Through their experiments, the authors showed that mining for evidences, and modeling them with statistical hypothesis tests, allows for an optimal fraud detector that can easily be extended both in this context and in other contexts. 


\section{The Human Element}

In this section I will discuss some game theory and behavioral elements of fraud and ways of eliminating frauds and fraudsters.



\noindent
%where $\omega$ is the frequency of the plasmon, $c$ is the speed of
%light, $\varepsilon_m$ is the dielectric constant of the metal,
%$\varepsilon_i$ is the dielectric constant of neighboring insulator,
%and $\varepsilon_{air}$ is the dielectric constant of air.
%Equation~\ref{eqn:sampleEqn} makes this perfectly clear.
%See also Figure~\ref{fig:myfig} for an illustration.
%
%
%\begin{figure}
%\centering
%\includegraphics[width=0.75\textwidth]{graph.png}
%\caption{My figure.}
%\label{fig:myfig}
%\end{figure}

\pagebreak
\chapter{ Fraud in Context}
\label{sec:context}

\section{ Interviews from Professionals}

This section will be about what professionals are doing in the fraud field.

\section{ Research Versus Reality}

This section I will compare whatever I find out from professionals to the most modern research on this topic. I assume research will be slightly behind what these professionals are dealing with and how they are solving these problems.

%(As promised in Chapter~\ref{sec:intro}, here it gets interesting.)

\pagebreak
\chapter{Implementing a Fraud Detection System}
\label{sec:impl}

To understand further the complexity of creating a highly accurate fraud detection system, I implement my own system based on past research discussed in Chapter~\ref{sec:background}. I construct a series of machine learning models, and compare the results based on different proportions of fraud to non-fraud data. All experimentation is done open source tools built for python, such as Scikit-learn and Numpy. I will discuss the model and the experiment in depth in the sections below. The overall implementation is broken down into subsections under Experiment: Hypothesis, Data, Methods, Results, and Discussion. 

\section{Model}

This section is where I will talk about the overall system I am planning on implementing.

\section{Experiment}

\subsection{Hypothesis}

\subsection{Data}

The data used from this project comes from a public source on Kaggle.com. I used the ``Synthetic Financial Datasets for Fraud Detection" which was generated by the PaySim mobile money simulator \cite{paysim}. This dataset comes from the paper ``PaySim: A financial mobile money simulator for fraud detection" and is scaled down to a quarter of the original dataset. The dataset includes information about type of the transaction, amount, the customer that started the transaction, the initial balance of the sender, the balance after the transaction of the sender, whether its fraud, and other variables. The summary statistics are shown in Table ~\ref{sec:sumstats}. 

% TODO: make summary statistic table, at least with the fraud/nonfraud distributions and attributes



\begin{table}[htbp]\centering
\def\sym#1{\ifmmode^{#1}\else\(^{#1}\)\fi}
\caption{Summary Statistics \label{tab1}}
\label{sec:sumstats}
%\vspace{-18 pt}

% %%%%%%%%%
\scalebox{0.8} {
\begin{tabular}{l c c c c c} \hline\hline
\addlinespace
\multicolumn{1}{c}{}&\multicolumn{4}{c}{2009 to 2015}\\
Variable&mean& s.d.&min& max\\
\hline

\addlinespace
\multicolumn{5}{l}{\textit{Measures of abortion access}}\\
Distance (hundreds of miles)	            &	0.967	&	0.72	&	0.003	&	3.12	\\
I(distance$<$50 miles) 	                &	0.28		&	0.45	&	0	&	1	\\
I(50$<$ Distance $\le$ 100)	            &	0.37		&	0.48	&	0	&	1	\\
I(100$<$ Distance $\le$ 150)	        &	0.16		&	0.37	&	0	&	1	\\
I(150$<$ Distance $\le$ 200)	        &	0.06		&	0.24	&	0	&	1	\\
I(200 $<$ Distance)	                    	&	0.12    	&	0.32	&	0	&	1	\\
Clinics						  &   0.144		& 	0.817	&	0.0		&	10\\
Average Service Population (100,000s)	&	2.01	&	1.19	&	0.144	&	5.73 \\

\addlinespace
\multicolumn{5}{l}{\textit{Measures of family planning access}}\\
Distance (hundreds of miles)	            &	0.39	&	0.401	&	0.002	&	2.63	\\
I(distance$<$50 miles) 	                &	0.75		&	0.43	&	0	&	1	\\
I(50$<$ Distance $\le$ 100)	            &	0.16		&	0.37	&	0	&	1	\\
I(100$<$ Distance $\le$ 150)	        &	0.05		&	0.22	&	0	&	1	\\
I(150$<$ Distance $\le$ 200)	        &	0.02		&	0.15	&	0	&	1	\\
I(200 $<$ Distance)	                    	&	0.01    	&	0.08	&	0	&	1	\\
Clinics						  &   0.75		& 	2.33	&	0.0		&	29\\

\addlinespace													
\multicolumn{5}{l}{\textit{Race}}\\													
White	     &	53.85	&	21.17	&	2.45	&	91.53\\
Black	     &	6.66	&	7.53	&	0	&	40.12	 \\
Hispanic	 &	37.65	&	23.57	&	2.79	&	97.03 \\
Other	     &	1.82	&	2.048	&	0	&	21.23	 \\

\addlinespace													
\multicolumn{5}{l}{\textit{Economic conditions}}\\
Median Income			& 4.88	&	1.12	&	2.33	&	9.15 \\
Poverty Rate			& 17.54	&	6.32	&	0	&	42.73 \\
Population in Urban Areas	 & 13.71	&	30.1	&	0	&	99.31 \\								
Unemployment rate  	&	6.435	&	2.27	&	2	&	18.5	\\

\addlinespace													
\multicolumn{5}{l}{\textit{Abuse Rates per 1000 Children}}\\										
Unconfirmed	 &	37.87	&	14.78	&	0	&	105.26 \\
Confirmed	     &	13.10	&	8.34	&	0	&	122.66	 \\
Total Victims	& 50.97	&	20.11	&	0	&	226.61 \\

\addlinespace													
\multicolumn{5}{l}{\textit{Population Statistics}}\\													
Child Population	 &	27,759.88	&	101,386.3	&	19	&	1,224,413 \\
Highschool Degree (Over 25)	     &	77.99	&	8.186	&	44.877	&	94.02	 \\
College Degree (Over 25)	     &	17.54	&	7.09	&	3.66	&	6671	 \\
\hline\hline
\end{tabular}
}
\end{table} 



\pagebreak




\subsection{Methods}

LOTS of Machine Learning incorporated into bigger fraud detection and rule based learning systems! Overall system will be discussed in Model section.

\subsection{Results}

This section will be full of tables and figures. There will probably also be an appendix with the entire set of experiments run. The most interesting or a subset of the experimental runs will be in a table in this section.

\subsection{Discussion}

This section will compare my hypothesis to the results, as well as my results to the model I'm basing it off of. I also plan on adding any notes I learn along the way of this implementation.
\pagebreak
\chapter{Further Applications}


\section{Anomoly Detection}

\subsection{Cell Phone Fraud}

\subsection{Biometric Fraud}

\subsection{Bioinformatics}

\pagebreak
\chapter{Conclusions}


%\appendix
%\chapter{Chapter 1 of appendix}
%Appendix chapter 1 text goes here

\bibliography{thesis}

\end{document}
